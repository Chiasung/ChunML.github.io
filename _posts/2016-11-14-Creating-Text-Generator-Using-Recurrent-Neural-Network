---
title: "Creating A Text Generator Using Recurrent Neural Network"
header:
  teaser: teaser.jpg
categories:
  - Project
tags:
  - machine-learning
  - deep-learning
  - keras
  - recurrent neural network
  - gpu
  - training
  - RNN
  - LSTM
  - GRU
  - text generator
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
Hello guys, it's been another while since my last post, and I hope you're all doing well with your own projects. I've been kept busy with my own stuff, too. And till this point, I got some interesting results which urged me to share to all you guys. Yeah, what I did is creating a Text Generator through training a Recurrent Neural Network Model.

Many of you may know about Recurrent Neural Networks, and many may not, but I'm quite sure that you all heard about Neural Networks. We have already seen how Neural Networks can solve nearly all Machine Learning problems no matter how complicated they are. And because to fully understand how Neural Networks work does require a lot of time for reading and implementing by yourself, and yet I haven't made any tutorials on them, it's nearly impossible to write it all in this post. So it'd be better to leave them for some future tutorials and make it easy this time by looking at the picture below instead. 

![neural_network](/images/projects/creating-text-generator-using-recurrent-neural-network/neural_network.png)

As you could see in the picture above, the main reason why Neural Network can out-perform other learning algorithms is because of the **hidden layers**. What the hidden layers do is to create a more complicated set of features, which results in a better predicting accuracy. I also mentioned about this in my previous posts: the more complicated and informative the features become, the more likely your Model can learn better and give more precise predictions.

With what we have seen in the last decade, we all might think that it's no longer until the human's brains are completely defeated by artificial intelligence. Yeah, that day will come, but luckily, it's still far away.  

You may ask why I'm so confident about that. Let's take a look at the picture below and guess what it is about:

![picture_t](/images/projects/creating-text-generator-using-recurrent-neural-network/picture_t.png)

Now you all have your own guess. Let's continue with the picture below. What if I tell you that what you see in the 2nd picture happened before the 1st picture?

![picture_t-1](/images/projects/creating-text-generator-using-recurrent-neural-network/picture_t-1.png)

I think you got it now. The way we understand things depends on ....

So now you could the limitation of Neural Networks and understand why they still cannot reach our brains' level (at least at this time). And of course, 

### Vanilla RNN

Hidden state

$$
h_t=f(W_{xh}x_t+W_{hh}h_{t-1})
$$

Output

$$
y_t=softmax(W_{hy}h_t)
$$

Long term dependencies problem

=> LSTM

### LSTM

Forget gate layer

$$
f_t=\sigma(W_f.\left[h_{t-1}, x_t\right]+b_f)
$$

Input gate layer

$$
i_t=\sigma(W_i.[h_{t-1}, x_t]+b_i)
$$

New candidate

$$
\tilde{C}_t=tanh(W_C.[h_{t-1},x_t]+b_C)
$$

Update old cell state to new cell state

$$
C_t=f_t*C_{t-1}+i_t*\tilde{C}_t
$$

Compute output

$$
o_t=\sigma(W_o[h_{t-1},x_t]+b_O)
$$

Hidden state

$$
h_t=o_t*tanh(C_t)
$$

Real output 

$$
y_t=softmax(W_{hy}h_t)
$$

### Another form: GRU

$$
z_t=\sigma(W_z.[h_{t-1},x_t]+b_z)
$$

$$
r_t=\sigma(W_r.[h_{t-1},x_t]+b_r)
$$

$$
\tilde{h}_t=tanh(W_h[r*h_{t-1},x_t]+b_h)
$$

$$
h_t=(1-z_t)*h_{t-1}+z_t*\tilde{h}_t
$$

### Implementation a Text Generator using RNN

As I mentioned earlier in this post, there are quite a lot of excellent posts on how Recurrent Neural Networks work, and those guys also included the implementations for demonstration. Actually, bcause they wrote code for teaching purpose, reading the codes does help understanding the tutorials a lot. But I must say that it may hurt, especially if you don't have any experience in Theano or Torch (Denny wrote his code in Theano and Andrej used Torch). I want to make it easy for you, so I will show you how to implement RNN using Keras, an excellent work from Fran√ßois Chollet, which I had a chance to introduced to you in my previous posts.

If you don't have Keras installed on your machine, just give the link below a click. The installation only takes 20 minutes (max):

* [Installing OpenCV & Keras](https://chunml.github.io/ChunML.github.io/tutorial/Setting-Up-Python-Environment-For-Computer-Vision-And-Machine-Learning/)

Now, let's get down to business

1. Prepare the training data

{% highlight python %} 
data = open('data/Harry Potter.txt', 'r').read()
chars = list(set(data))

ix_to_char = {ix:char for ix, char in enumerate(chars)}
char_to_ix = {char:ix for ix, char in enumerate(chars)}
{% endhighlight %}

2. Create the Network

{% highlight python %} 
BATCH_SIZE = args['batch_size']
HIDDEN_DIM = args['hidden_dim']
SEQ_LENGTH = args['seq_length']
VOCAB_SIZE = len(chars)
GENERATE_LENGTH = args['generate_length']
LAYER_NUM = args['layer_num']
{% endhighlight %}

{% highlight python %} 
model = Sequential()
model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))
for i in range(LAYER_NUM - 1):
    model.add(LSTM(HIDDEN_DIM, return_sequences=True))
model.add(TimeDistributed(Dense(VOCAB_SIZE)))
model.add(Activation('softmax'))
model.compile(loss="categorical_crossentropy", optimizer="rmsprop")
{% endhighlight %}

3. Train the Network


{% highlight python %} 
nb_epoch = 40
while True:
        X = np.zeros((len(data)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE))
	y = np.zeros((len(data)/SEQ_LENGTH, SEQ_LENGTH, VOCAB_SIZE))

	for i in range(0, len(data)/SEQ_LENGTH):

		X_sequence = data[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]
		X_sequence_ix = [char_to_ix[value] for value in X_sequence]
		input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))
		for j in range(SEQ_LENGTH):
			input_sequence[j][X_sequence_ix[j]] = 1
		X[i] = input_sequence

		y_sequence = data[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]
		y_sequence_ix = [char_to_ix[value] for value in y_sequence]
		target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))
		for j in range(SEQ_LENGTH):
			target_sequence[j][y_sequence_ix[j]] = 1.
		y[i] = target_sequence

        model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, nb_epoch=1)
        nb_epoch += 1
        generate_text(model, GENERATE_LENGTH)
        if nb_epoch % 10 == 0:
            model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, nb_epoch))
{% endhighlight %}

4. Generate text

{% highlight python %} 
def generate_text(model, length):
    ix = [np.random.randint(VOCAB_SIZE)]
    y_char = [ix_to_char[ix[-1]]]
    X = np.zeros((1, length, VOCAB_SIZE))
    for i in range(length):
        X[0, i, :][ix[-1]] = 1
        print(ix_to_char[ix[-1]], end="")
        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)
        y_char.append(ix_to_char[ix[-1]])
    return ('').join(y_char)
{% endhighlight %}

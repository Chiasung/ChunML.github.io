---
title: "Sequence To Sequence"
header:
  teaser: projects/sequence-to-sequence/
categories:
  - Project
tags:
  - machine-learning
  - deep-learning
  - keras
  - recurrent neural network
  - gpu
  - training
  - RNN
  - LSTM
  - GRU
  - seq2seq
  - translator model
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
Hello guys. It's been quite a long while since my last tutorial blogpost. It may sound like an excuse, but I've been struggling with finding a new place to live. And I had to say, it's a real problem for a foreigner to find a reasonable apartment in Japan. Luckily, I somehow managed to find one, and I have just moved in for nearly a week. Anyway, the hardest time has gone, and now I can get back to work, to bring to you guys new interesting (and maybe boring as usual) blog posts on Deep Learning.

So, in the last post, I told you about how to create a simple text generator by training a Recurrent Neural Networks. What RNNs differ from normal Neural Networks is, instead of computing the output prediction on each input independently, RNNs compute the output of timestep \\(t\\) using not only the input of timestep \\(t\\), but also involving the input of previous timesteps (say, timestep \\(t-1\\), \\(t-2\\), \\(\dots\\)). 

As you already saw in my previous post, inputs are sequences of characters, and each output was simply the corresponding input shifted by one character to the right. Obviously, you can see that each pair of input sequence and output sequence has the same length. Then, the network was trained using the famous Harry Potter as training dataset and as a result, the trained model could generate some great J.K. Rowling-alike paragraphs. If you haven't read my previous post yet, please refer to it at the link below (make sure you do before moving on):

[Creating A Text Generator Using Recurrent Neural Network](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/)

But here comes a big question: What if input sequence and output sequence have different lengths?

You know, there are many types of Machine Learning problems out there where input and output sequences don't necessarily have the exact same lengths. And in terms of Natural Language Processing (or NLP for short), you are more likely to face the problems where the lengths are totally different, not only between each pair of input and output sequence, but also between input sequences themselves! For example, in building a language translation model, each pair of input and output sequence are in different languages, so there's a big chance that they don't have the same length. Moreover, I can bet my life that there is no known language where we can create all sentences with the exact same length! Obviously, that is a really big big problem, because the model I showed you in the last post required all the input and output sequences have the same length. Sounds impossible, huh?

The answer is: NO. Big problems only got smart people attracted, and as a result, solved by not only one, but many solutions. Let's go back to our problem. A lot of attempts were made, each of them has its own advantages and disadvantages when compared to others. And in today's post, I will introduce to you one approach which received great attention from NLP community: The Sequence To Sequence Networks (or seq2seq for short), great work by Ilya Sutskever, Oriol Vinyals, Quoc V. Le from Google.

I will talk briefly about the idea behind seq2seq right below. For ones who want to understand deeply about the state-of-the-art model, please refer to the link to the paper at the end of this post.

At this point, we have already known the problem we must deal with, that we have input and output sequences of different lengths. To make the problem become more concrete, let's take a look at the graph below:

![figure](/images/projects/sequence-to-sequence/figure.png)  
(Image cut from the original paper of Sequence to Sequence Learning with Neural Networks)

As illustrated in the graph above, we have "ABC" as the input sequence, and "WXYZ" as the output sequence. Obviously, the lengths of the two sequences are different. So, how does seq2seq approach to solve that problem? The answer is: they create a model which consists of two seperate recurrent neural networks called **Encoder** and **Decoder** respectively. To make it easy for you, I drew a simple graph below:

![encode_decode](/images/projects/sequence-to-sequence/encode_decode.png)

As the names of the two networks are somehow self-explained, first, it's clear that we can't directly compute the output sequence by using just one network, so we need to use the first network to **encode** the input sequence into some kind of "middle sequence", then the other network will decode that sequence into our desire output sequence. So, what does the "middle sequence" look like? Let's take a look at the next graph below:

![repeated_vec](/images/projects/sequence-to-sequence/repeated_vec.png)

The mystery was revealed! Concretely, what the Encoder actually did is creating a temporary output vector from the input sequence (you can think about that temporary output vector as a sequence with only one timestep). Then, that vector is repeated \\(n\\) times, with \\(n\\) is the length of our desire output sequence. Up to this point, you may get all the rest. Yep, the Decoder network acts exact the same way with the network I talked about in the last post. After repeating the output vector from the Encoder \\(n\\) times, we obtain a sequence with exact the same length with the associated output sequence, we can leave the computation for the Decoder network! And that's the idea behind seq2seq. It's not as hard as it might seem, right?

### Reference

* Ilya Sutskever, Oriol Vinyals and Quoc V. Le. [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

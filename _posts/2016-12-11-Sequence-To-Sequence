---
title: "Creating A Sequence To Sequence Learning Model"
header:
  teaser: projects/sequence-to-sequence/
categories:
  - Project
tags:
  - machine-learning
  - deep-learning
  - keras
  - recurrent neural network
  - gpu
  - training
  - RNN
  - LSTM
  - GRU
  - seq2seq
  - translator model
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
Hello guys. It's been quite a long while since my last tutorial blogpost. It may sound like an excuse, but I've been struggling with finding a new place to live. And I had to say, it's a real problem for a foreigner to find a reasonable apartment in Japan. Luckily, I somehow managed to find one, and I have just moved in for nearly a week. Anyway, the hardest time has gone, and now I can get back to work, to bring to you guys new interesting (and maybe boring as usual) blog posts on Deep Learning.

So, in the last post, I told you about how to create a simple text generator by training a Recurrent Neural Networks. What RNNs differ from normal Neural Networks is, instead of computing the output prediction on each input independently, RNNs compute the output of timestep \\(t\\) using not only the input of timestep \\(t\\), but also involving the input of previous timesteps (say, timestep \\(t-1\\), \\(t-2\\), \\(\dots\\)). 

As you already saw in my previous post, inputs are sequences of characters, and each output was simply the corresponding input shifted by one character to the right. Obviously, you can see that each pair of input sequence and output sequence has the same length. Then, the network was trained using the famous Harry Potter as training dataset and as a result, the trained model could generate some great J.K. Rowling-alike paragraphs. If you haven't read my previous post yet, please refer to it at the link below (make sure you do before moving on):

[Creating A Text Generator Using Recurrent Neural Network](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/)

But here comes a big question: What if input sequence and output sequence have different lengths?

You know, there are many types of Machine Learning problems out there where input and output sequences don't necessarily have the exact same lengths. And in terms of Natural Language Processing (or NLP for short), you are more likely to face the problems where the lengths are totally different, not only between each pair of input and output sequence, but also between input sequences themselves! For example, in building a language translation model, each pair of input and output sequence are in different languages, so there's a big chance that they don't have the same length. Moreover, I can bet my life that there is no known language where we can create all sentences with the exact same length! Obviously, that is a really big big problem, because the model I showed you in the last post required all the input and output sequences have the same length. Sounds impossible, huh?

The answer is: NO. Big problems only got smart people attracted, and as a result, solved by not only one, but many solutions. Let's go back to our problem. A lot of attempts were made, each of them has its own advantages and disadvantages when compared to others. And in today's post, I will introduce to you one approach which received great attention from NLP community: The Sequence To Sequence Networks (or seq2seq for short), great work by Ilya Sutskever, Oriol Vinyals, Quoc V. Le from Google.

I will talk briefly about the idea behind seq2seq right below. For ones who want to understand deeply about the state-of-the-art model, please refer to the link to the paper at the end of this post.

At this point, we have already known the problem we must deal with, that we have input and output sequences of different lengths. To make the problem become more concrete, let's take a look at the graph below:

![figure](/images/projects/sequence-to-sequence/figure.png)  
(Image cut from the original paper of Sequence to Sequence Learning with Neural Networks)

As illustrated in the graph above, we have "ABC" as the input sequence, and "WXYZ" as the output sequence. Obviously, the lengths of the two sequences are different. So, how does seq2seq approach to solve that problem? The answer is: they create a model which consists of two seperate recurrent neural networks called **Encoder** and **Decoder** respectively. To make it easy for you, I drew a simple graph below:

![encode_decode](/images/projects/sequence-to-sequence/encode_decode.png)

As the names of the two networks are somehow self-explained, first, it's clear that we can't directly compute the output sequence by using just one network, so we need to use the first network to **encode** the input sequence into some kind of "middle sequence", then the other network will decode that sequence into our desire output sequence. So, what does the "middle sequence" look like? Let's take a look at the next graph below:

![repeated_vec](/images/projects/sequence-to-sequence/repeated_vector.png)

The mystery was revealed! Concretely, what the Encoder actually did is creating a temporary output vector from the input sequence (you can think about that temporary output vector as a sequence with only one timestep). Then, that vector is repeated \\(n\\) times, with \\(n\\) is the length of our desire output sequence. Up to this point, you may get all the rest. Yep, the Decoder network acts exact the same way with the network I talked about in the last post. After repeating the output vector from the Encoder \\(n\\) times, we obtain a sequence with exact the same length with the associated output sequence, we can leave the computation for the Decoder network! And that's the idea behind seq2seq. It's not as hard as it might seem, right?

So we now know about how to output a sequence from an input of different length. But what about the lengths of input sequences? As I mentioned above, the input sequences themselves don't necessarily have the exact same length, either! Sounds like an other headache, doesn't it? Fortunately, it's far more relaxing than the problem above. In fact, all we need to do is just something called: **Zero Padding**. To make it easier for you to understand, let's see the image below:

![five_sentences](/images/projects/sequence-to-sequence/five_sentences.png)

Here I prepared three sentences and let's imagine that they will be the input sequences to our network. As you could see, three sentences are not equal in length. To make them all equal in length, let's take the length of the longest sentence as the common length, and we only need to add one same word some times to the end of the other two, until they have the same length as the longest one. The added word must not resemble any words in the sentences, since it will cause their meaning to change. I will use the word "ZERO", and here's the result I received:

![ZERO_added](/images/projects/sequence-to-sequence/ZERO_added.png)

You might get this now. And that's why it is called **zero padding**. In fact, what I did above is not exactly zero padding, and we will likely implement it differently. I'll tell you more in the Implementation section. For now, all I wanted to do is just to help you understand zero padding without any hurt.

We are half way there! We now know all we need to know about the-state-of-the-art Sequence to Sequence Learning. I can't help jumping right into Implementation section. Neither can you, right?

### Implementation

So, now we are here, finally, right in the Implementation section. Working with NLP problems is literally abstract (than what we did in Computer Vision problems, which we could at least have some visualization). Even worse, deep neural network in common is kind of abstract itself, so it seems that thing's gonna get more complicated here. That's the reason why I decided not to dig into details in the previous section, but to explain it along with the corresponding part in the code instead so that you won't find it difficult to understand the abstract terms (at least I think so). And now, let's get your hands dirty!

As usual, we will start with the most tedious (and boring) but important task, which is **Data Preparation**. As you already saw in my previous post, it is a little bit more complicated to prepare language data rather than image data. You will understand why soon.

Before doing any complicated processing, first we need to read the training data from file. I defined *X_data* and *y_data* variables to store the input text and the output text, respectively. They are all raw string objects, which means that we must split them into sentences:

{% highlight python %}
X = [text_to_word_sequence(x)[::-1] for x, y in zip(X_data.split('\n'), y_data.split('\n')) if len(x) > 0 and len(y) > 0 and len(x) <= max_len and len(y) <= max_len]
y = [text_to_word_sequence(y) for x, y in zip(X_data.split('\n'), y_data.split('\n')) if len(x) > 0 and len(y) > 0 and len(x) <= max_len and len(y) <= max_len]
{% endhighlight %}

Let's break it down for a better understanding. I will use the three sentences above as our *X_data*, here's what happened after *X_data.split('\n')*

![split_sentences](/images/projects/sequence-to-sequence/split_sentences.png)

The easiest way to split a raw text into sentences is looking for the line break. Of course, there are many other better ways, but let's make it simple this time. So, from a raw text we now obtained an array of sentences.

Next, for each sentence in the array, we must then split it into an array of words, or say it in a more proper way, a sequence of words. We will do this by using Keras' predefined method called **text_to_word_sentence**, as illustrated below:

![split_words](/images/projects/sequence-to-sequence/split_words.png)

Splitting a sentence into a sequence of words is harder than splitting text into sentences, since there are many ways to seperate words in a sentence, e.g. spaces, commas and so on. So we should not self-implement it but make use of predefined method instead. **text_to_word_sentence** also helps us remove all the sentence ending marks such as periods or exclamation marks. Quite helpful, isn't it?

So, here's what we received, an array of sequences of words:

![sequences](/images/projects/sequence-to-sequence/sequences.png)

But wait! There's one minor change which needs to be made to the input sequences, as mentioned from the paper as follow:

> We found
it extremely valuable to reverse the order of the words of the input sentence. So for example, instead
of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ,
where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to
β, and so on, a fact that makes it easy for SGD to “establish communication” between the input and
the output. We found this simple data transformation to greatly boost the performance of the LSTM.

If you noticed the graph I drew above, you would have some doubt about the order of the input sequence. Yeah, as you might guess, the order of the input sequence is reversed before going into the network. And that's the reason why I added **[::-1]** to reverse the sequence split from the raw text. So, the final input sequences look like below:

![reverse_sequences](/images/projects/sequence-to-sequence/reverse_sequences.png)

Seems like we're done, right? But sadly, we are only half way there before we can actually have the network train our data. As computers can only understand the gray scale values of pixels in an image, inputting sequences of raw human-alike words will make no sense to computers. For that reason, we need to take a further step, which is converting the raw words into some kind of numeric values. To do that, we need a dictionary to map from a word to its corresponding index value, and another dictionary for the same purpose, but in reverse direction.

But first, what we need is a vocaburaly set. You can think of vocabulary set as an array which stores all the words in the raw text, but each word only appears once.

{% highlight python %}
dist = FreqDist(np.hstack(X))
X_vocab = dist.most_common(vocab_size-1)
dist = FreqDist(np.hstack(y))
y_vocab = dist.most_common(vocab_size-1)
{% endhighlight %}

In real deep learning projects, especially when we're dealing with NLP problems, our training data is pretty large in size, which the number of vocabularies may be up to millions. Obviously, that's too much for our computers to handle. Furthermore, words which appear only a few times (typically once or twice) in the whole text may not have a significant impact on the learning of our network. So, what we do first is to count the frequency which a word appears in the text, then we create the vocabulary set using only 10000 words with highest frequencies (you can change to 20000 or more, but make sure that your machine can handle it).

The result may look like below:

![vocab_set](/images/projects/sequence-to-sequence/vocab_set.png)

So we just have created the vocabulary set from the input text. In the next step, we will create two dictionaries to map between each word and its index in the vocabulary set, and vice versa.

{% highlight python %}
# Create an array of words from the vocabulary set, we will use this array as index-to-word dictionary
X_ix_to_word = [word[0] for word in X_vocab]
# Add the special word 'ZERO' to the beginning of the array
X_ix_to_word.insert(0, 'ZERO')
# Add the special word 'UNK' to the end of the array (stands for UNKNOWN word)
X_ix_to_word.append('UNK')

![ix_to_word](/images/projects/sequence-to-sequence/ix_to_word.png)

# Create the word-to-index dictionary from the array created above
X_word_to_ix = {word:ix for ix, word in enumerate(X_ix_to_word)}
{% endhighlight %}

![word_to_ix](/images/projects/sequence-to-sequence/word_to_ix.png)

Add some more explanation here

So now we got the two dictionaries ready. The next step is pretty simple: we will loop through the sequences and replace every word in each sequence by its corresponding index number. And also remember that we're only putting 10000 words with highest frequencies into the vocabulary set, which also means that our network will actually learn words from that vocabulary set only. So here comes the question: What happens to the other words and how can we converse them to numeric values? That's where the word **UNK** makes sense. It stands for "Unknown words", or it's sometimes called **OOV**, which means "Out Of Vocabulary". So, for words which are not in the vocabulary set, we will simply assign them as "UNK". And as you may guess, they will all have the same index value.

{% highlight python %}
for i, sentence in enumerate(X):
    for j, word in enumerate(sentence):
        if word in X_word_to_ix:
            X[i][j] = X_word_to_ix[word]
        else:
            X[i][j] = X_word_to_ix['UNK']
{% endhighlight %}

![index_sequence](/images/projects/sequence-to-sequence/index_sequence.png)

- Zero padding
{% highlight python %}
X = pad_sequences(X, maxlen=X_max_len, dtype='uint8')
y = pad_sequences(y, maxlen=y_max_len, dtype='uint8')
{% endhighlight %}

![zero_pad_index_sequence](/images/projects/sequence-to-sequence/zero_pad_index_sequence.png)

- Vectorization
{% highlight python %}
sequences = np.zeros((len(word_sentences), max_len, len(word_to_ix)))
for i, sentence in enumerate(word_sentences):
    for j, word in enumerate(sentence):
        sequences[i, j, word] = 1.
{% endhighlight %}

![vectorization](/images/projects/sequence-to-sequence/vectorization.png)

- Create encoder network
{% highlight python %}
model = Sequential()
model.add(Embedding(X_vocab_len, 1000, input_length=X_max_len, mask_zero=True))
model.add(LSTM(hidden_size))
{% endhighlight %}

- Create decoder network
{% highlight python %}
model.add(RepeatVector(y_max_len))
for _ in range(num_layers):
    model.add(LSTM(hidden_size, return_sequences=True))
model.add(TimeDistributed(Dense(y_vocab_len)))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy',
            optimizer='rmsprop',
            metrics=['accuracy'])
{% endhighlight %}

- Traing the seq2seq
{% highlight python %}
for k in range(1, NB_EPOCH+1):
    indices = np.arange(len(X))
    np.random.shuffle(indices)
    X = X[indices]
    y = y[indices]
    for i in range(0, len(X), 1000):
        if i + 1000 >= len(X):
            i_end = len(X)
        else:
            i_end = i + 1000
        y_sequences = process_data(y[i:i_end], y_max_len, y_word_to_ix)

        print('[INFO] Training model: epoch {}th {}/{} samples'.format(k, i, len(X)))
        model.fit(X[i:i_end], y_sequences, batch_size=BATCH_SIZE, nb_epoch=1, verbose=2)
    model.save_weights('checkpoint_epoch_{}.hdf5'.format(k))
{% endhighlight %}
